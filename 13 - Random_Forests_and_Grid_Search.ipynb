{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('diabetes.csv')\n",
    "X = df.drop('Outcome',axis=1)\n",
    "y = df['Outcome']\n",
    "X_col=X.columns\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and test sets\n",
    "X_train, X_test , y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1,stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit 100 trees and bag them\n",
    "tree = DecisionTreeClassifier()\n",
    "bag = BaggingClassifier(tree, n_estimators=100, random_state=0,oob_score=True)\n",
    "bag.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=bag.predict(X_test)\n",
    "print('Test Accuracy:', accuracy_score(y_test, y_pred),'Oob Accuracy:',bag.oob_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit random forests\n",
    "clf=RandomForestClassifier(random_state=0,n_estimators=100,oob_score=True)\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred=clf.predict(X_test)\n",
    "print('Test Accuracy:', accuracy_score(y_test, y_pred),'Oob Accuracy:',clf.oob_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's try different numbers of trees\n",
    "Oob_Accuracy=[]\n",
    "for i in np.linspace(start = 50, stop = 500, num = 10):\n",
    "    clf=RandomForestClassifier(random_state=0,n_estimators=int(i),oob_score=True)\n",
    "    clf.fit(X_train,y_train)\n",
    "    Oob_Accuracy.append([i,np.array(clf.oob_score_)])\n",
    "df = pd.DataFrame(Oob_Accuracy,columns=['Number_of_Trees','Oob Accuracy'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.plot(df['Number_of_Trees'].values,df['Oob Accuracy'].values,label = 'Oob Accuracy')\n",
    "ax.set_xlabel('Number_of_Trees')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.tick_params(axis='x', labelsize=8)\n",
    "ax.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Although we picked 350, we could have picked a bigger number. Higher Number of trees do not lead to overfitting\n",
    "clf=RandomForestClassifier(random_state=0,n_estimators=350,oob_score=True)\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred=clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's try different numbers of features at each split (default is sqrt(p) where p is the number of features)\n",
    "Oob_Accuracy=[]\n",
    "for i in range(1,9):\n",
    "    clf=RandomForestClassifier(random_state=0,n_estimators=350,max_features=i,oob_score=True)\n",
    "    clf.fit(X_train,y_train)\n",
    "    Oob_Accuracy.append([i,np.array(clf.oob_score_)])\n",
    "df = pd.DataFrame(Oob_Accuracy,columns=['Number_of_Features','Oob Accuracy'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=RandomForestClassifier(random_state=0,n_estimators=350,max_features=3,oob_score=True)\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred=clf.predict(X_test)\n",
    "print('Test Accuracy:', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and test sets\n",
    "X_train, X_test , y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10,stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Oob_Accuracy=[]\n",
    "for i in range(1,9):\n",
    "    clf=RandomForestClassifier(random_state=0,n_estimators=350,max_features=i,oob_score=True)\n",
    "    clf.fit(X_train,y_train)\n",
    "    y_pred=clf.predict(X_test)\n",
    "    Oob_Accuracy.append([i,np.array(clf.oob_score_)])\n",
    "df = pd.DataFrame(Oob_Accuracy,columns=['Number_of_Features','Oob Accuracy'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=RandomForestClassifier(random_state=0,n_estimators=350,max_features=6,oob_score=True)\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred=clf.predict(X_test)\n",
    "print('Test Accuracy:', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=RandomForestClassifier(random_state=0)\n",
    "# number of trees in random forest\n",
    "n_estimators = [100,200,300,400,500,600]\n",
    "# number of features at every split\n",
    "max_features = [1,2,3,4,5,6,7,8]\n",
    "# create grid\n",
    "params = {\n",
    " 'n_estimators': n_estimators,\n",
    " 'max_features': max_features,\n",
    " }\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search of parameters\n",
    "clf_grid = GridSearchCV(estimator = clf, param_grid = params, \n",
    "                                cv = 5, verbose=2, scoring='accuracy',n_jobs = -1)\n",
    "# Fit the model\n",
    "clf_grid.fit(X_train, y_train)\n",
    "# print results\n",
    "print(clf_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=RandomForestClassifier(random_state=0,n_estimators=300,max_features=4,oob_score=True)\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred=clf.predict(X_test)\n",
    "print('Test Accuracy:', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and test sets\n",
    "X_train, X_test , y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1,stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search of parameters\n",
    "clf_grid = GridSearchCV(estimator = clf, param_grid = params, \n",
    "                                cv = 5, verbose=2, scoring='accuracy',n_jobs = -1)\n",
    "# Fit the model\n",
    "clf_grid.fit(X_train, y_train)\n",
    "# print results\n",
    "print(clf_grid.best_params_)\n",
    "#The results are different :) This will always happen because there is variability due to a change in training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=RandomForestClassifier(random_state=0,n_estimators=100,max_features=6,oob_score=True)\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred=clf.predict(X_test)\n",
    "print('Test Accuracy:', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=RandomForestClassifier(random_state=0)\n",
    "# number of trees in random forest\n",
    "n_estimators = [100,200,300,400,500,600]\n",
    "# number of features at every split\n",
    "max_features = [1,2,3,4,5,6,7,8]\n",
    "# create grid\n",
    "params = {\n",
    " 'n_estimators': n_estimators,\n",
    " 'max_features': max_features,\n",
    " }\n",
    "#Instead of one test set we will use cross validation, please note that both parameter selection and \n",
    "#the test performance is computed via cross validation (a nested cross-validation) \n",
    "from sklearn.model_selection import KFold\n",
    "cv = KFold(n_splits=5, random_state=1, shuffle=True)\n",
    "CVErrors=[]\n",
    "for train_index, validation_index in cv.split(X):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[validation_index], \n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[validation_index]\n",
    "    # Grid search of parameters\n",
    "    clf_grid = GridSearchCV(estimator = clf, param_grid = params, \n",
    "                                cv = 5, verbose=2, scoring='accuracy',n_jobs = -1)\n",
    "    # Fit the model\n",
    "    clf_grid.fit(X_train, y_train)\n",
    "    # print results\n",
    "    print(clf_grid.best_params_)\n",
    "    #After finding best parameters fit the model\n",
    "    clf=RandomForestClassifier(**clf_grid.best_params_)\n",
    "    clf.fit(X_train,y_train)\n",
    "    y_pred=clf.predict(X_test)\n",
    "    #Test the performance on the test set\n",
    "    CVErrors.append(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CVErrors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(CVErrors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and test sets\n",
    "X_train, X_test , y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1,stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GridSearchCV()\n",
    "# Grid search of parameters\n",
    "clf_grid = GridSearchCV(estimator = clf, param_grid = params, \n",
    "                                cv = 5, verbose=2, scoring='roc_auc',n_jobs = -1)\n",
    "# Fit the model\n",
    "clf_grid.fit(X_train, y_train)\n",
    "# print results\n",
    "print(clf_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=RandomForestClassifier(random_state=0,n_estimators=400,max_features=2,oob_score=True)\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred=clf.predict(X_test)\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=RandomForestClassifier(random_state=0,n_estimators=400,max_features=2,oob_score=True)\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred_proba=np.array(clf.predict_proba(X_test))\n",
    "y_pred_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba[:,1])\n",
    "auc = roc_auc_score(y_test, y_pred_proba[:,1])\n",
    "plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n",
    "plt.legend(loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the impurity-based feature importances of the forest\n",
    "feats = {} # a dict to hold feature_name: feature_importance\n",
    "importances = clf.feature_importances_\n",
    "clf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature, importance in zip(X_col, clf.feature_importances_):\n",
    "    feats[feature] = importance #add the name/value pair \n",
    "importances = pd.DataFrame.from_dict(feats, orient='index').rename(columns={0: 'Gini-importance'})\n",
    "importances = importances.sort_values(by='Gini-importance',ascending=False)\n",
    "importances.plot.barh(color='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=LogisticRegression(solver='liblinear')\n",
    "C_param_range = [0.001,0.01,0.1,1,10,100,1000,10000]\n",
    "penalties=['l1','l2']\n",
    "# create grid\n",
    "params = {\n",
    " 'C': C_param_range,\n",
    " 'penalty': penalties,\n",
    " }\n",
    "\n",
    "clf_grid = GridSearchCV(estimator = clf, param_grid = params, \n",
    "                                cv = 5, verbose=2, scoring='roc_auc',n_jobs = -1)\n",
    "# Fit the model\n",
    "clf_grid.fit(X_train, y_train)\n",
    "# print results\n",
    "print(clf_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=LogisticRegression(C=100,penalty='l2',solver='liblinear')\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred=clf.predict(X_test)\n",
    "print('Test Accuracy:', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Hitters_Data.csv')\n",
    "df=df.dropna()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies = pd.get_dummies(df[['League', 'Division', 'NewLeague']])\n",
    "y = np.log(df.Salary)\n",
    "\n",
    "# Drop the column with the independent variable (Salary), and columns for which we created dummy variables\n",
    "X_ = df.drop(['Salary', 'League', 'Division', 'NewLeague'], axis = 1).astype('float64')\n",
    "\n",
    "# Define the feature set X.\n",
    "X = pd.concat([X_, dummies[['League_N', 'Division_W', 'NewLeague_N']]], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and test sets\n",
    "X_train, X_test , y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "scaler = preprocessing.StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor=RandomForestRegressor(random_state=0)\n",
    "RandomForestRegressor()\n",
    "# number of trees in random forest\n",
    "n_estimators = [100,200,300,400,500,600]\n",
    "# number of features at every split\n",
    "max_features = [3,4,5,6,7]\n",
    "# create grid\n",
    "params = {\n",
    " 'n_estimators': n_estimators,\n",
    " 'max_features': max_features,\n",
    " }\n",
    "# Random search of parameters\n",
    "clf_grid = GridSearchCV(estimator = regressor, param_grid = params, \n",
    "                                cv = 5, verbose=2, scoring='neg_mean_squared_error',n_jobs = -1)\n",
    "# Fit the model\n",
    "clf_grid.fit(X_train, y_train)\n",
    "# print results\n",
    "print(clf_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf=RandomForestRegressor(random_state=0,n_estimators=300,max_features=4)\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred=clf.predict(X_test)\n",
    "print('Mean Squared Error:', mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "# Create a pipeline\n",
    "pipe = Pipeline([('Regressor', RandomForestRegressor())])\n",
    "\n",
    "# Create space of candidate learning algorithms and their hyperparameters\n",
    "search_space = [{'Regressor': [Ridge()],\n",
    "                 'Regressor__alpha': np.logspace(-3, 1, 10)},\n",
    "                {'Regressor': [Lasso(max_iter = 10000)],\n",
    "                 'Regressor__alpha': np.logspace(-3, 1, 10)},\n",
    "                {'Regressor': [KNeighborsRegressor()],\n",
    "                 'Regressor__n_neighbors':[2,3,4,5,6]},\n",
    "                {'Regressor': [RandomForestRegressor(random_state=0)],\n",
    "                 'Regressor__n_estimators': [100, 200,300,400,500],\n",
    "                 'Regressor__max_features': [3,4,5,6,7]}]\n",
    "\n",
    "# Create grid search \n",
    "clf = GridSearchCV(pipe, search_space, cv=5, verbose=0)\n",
    "# Fit grid search\n",
    "best_model = clf.fit(X_train, y_train)\n",
    "# View best model\n",
    "best_model.best_estimator_.get_params()['Regressor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=best_model.predict(X_test)\n",
    "print('Mean Squared Error:', mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
