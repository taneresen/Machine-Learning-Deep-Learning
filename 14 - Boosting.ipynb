{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting with Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('diabetes.csv')\n",
    "X = df.drop('Outcome',axis=1)\n",
    "y = df['Outcome']\n",
    "X_col=X.columns\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and test sets\n",
    "X_train, X_test , y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1,stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "model = GradientBoostingClassifier(n_estimators=1000,max_depth=1,learning_rate=1)\n",
    "model.fit(X_train,y_train)\n",
    "y_pred=model.predict(X_test)\n",
    "print('Accuracy:', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# There is a trade-off between number of trees and learning rate. \n",
    "# You should never check the test performance when tuning parameters!!! This is only for illustrative purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GradientBoostingClassifier(n_estimators=1000,max_depth=1,learning_rate=0.1)\n",
    "model.fit(X_train,y_train)\n",
    "y_pred=model.predict(X_test)\n",
    "print('Accuracy:', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GradientBoostingClassifier(n_estimators=1000,max_depth=1,learning_rate=0.01)\n",
    "model.fit(X_train,y_train)\n",
    "y_pred=model.predict(X_test)\n",
    "print('Accuracy:', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GradientBoostingClassifier(n_estimators=5000,max_depth=1,learning_rate=0.01)\n",
    "model.fit(X_train,y_train)\n",
    "y_pred=model.predict(X_test)\n",
    "print('Accuracy:', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GradientBoostingClassifier(max_depth=1)\n",
    "#number of trees fit\n",
    "n_estimators = [100,1000,5000,10000]\n",
    "#learning rate\n",
    "learning_rate = [1,0.1,0.01,0.001]\n",
    "# create grid\n",
    "params = {\n",
    " 'n_estimators': n_estimators,\n",
    " 'learning_rate': learning_rate,\n",
    " }\n",
    "# Random search of parameters\n",
    "boost_grid = GridSearchCV(estimator = model, param_grid = params, \n",
    "                                cv = 5, verbose=2, scoring='accuracy',n_jobs = -1)\n",
    "# Fit the model\n",
    "boost_grid.fit(X_train, y_train)\n",
    "# print results\n",
    "print(boost_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=GradientBoostingClassifier(n_estimators=5000,max_depth=1,learning_rate=0.01)\n",
    "model.fit(X_train,y_train)\n",
    "y_pred=model.predict(X_test)\n",
    "print('Accuracy:', accuracy_score(y_test, y_pred))\n",
    "GradientBoostingClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaboost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "model = AdaBoostClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = [50,100,1000,5000]\n",
    "# number of features at every split\n",
    "learning_rate = [1,0.1,0.01,0.001]\n",
    "# create grid\n",
    "params = {\n",
    " 'n_estimators': n_estimators,\n",
    " 'learning_rate': learning_rate,\n",
    " }\n",
    "# Random search of parameters\n",
    "boost_grid = GridSearchCV(estimator = model, param_grid = params, \n",
    "                                cv = 5, verbose=2, scoring='accuracy',n_jobs = -1)\n",
    "# Fit the model\n",
    "boost_grid.fit(X_train, y_train)\n",
    "# print results\n",
    "print(boost_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=AdaBoostClassifier(n_estimators=100,learning_rate=0.1)\n",
    "model.fit(X_train,y_train)\n",
    "y_pred=model.predict(X_test)\n",
    "print('Accuracy:', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extreme Gradient Booster: A popular library that implements boosting faster and generally provides better performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "model = XGBClassifier()\n",
    "n_estimators = [50,100,1000,5000]\n",
    "# number of features at every split\n",
    "learning_rate = [1,0.1,0.01,0.001]\n",
    "# create grid\n",
    "params = {\n",
    " 'n_estimators': n_estimators,\n",
    " 'learning_rate': learning_rate,\n",
    " }\n",
    "# Random search of parameters\n",
    "boost_grid = GridSearchCV(estimator = model, param_grid = params, \n",
    "                                cv = 5, verbose=2, scoring='accuracy',n_jobs = -1)\n",
    "# Fit the model\n",
    "boost_grid.fit(X_train, y_train)\n",
    "# print results\n",
    "print(boost_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=XGBClassifier(n_estimators=100,learning_rate=0.01)\n",
    "model.fit(X_train,y_train)\n",
    "y_pred=model.predict(X_test)\n",
    "print('Accuracy:', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression with Gradient boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Hitters_Data.csv')\n",
    "df=df.dropna()\n",
    "dummies = pd.get_dummies(df[['League', 'Division', 'NewLeague']])\n",
    "y = np.log(df.Salary)\n",
    "\n",
    "# Drop the column with the independent variable (Salary), and columns for which we created dummy variables\n",
    "X_ = df.drop(['Salary', 'League', 'Division', 'NewLeague'], axis = 1).astype('float64')\n",
    "\n",
    "# Define the feature set X.\n",
    "X = pd.concat([X_, dummies[['League_N', 'Division_W', 'NewLeague_N']]], axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and test sets\n",
    "X_train, X_test , y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "scaler = preprocessing.StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "model = GradientBoostingRegressor(max_depth=1)\n",
    "n_estimators = [100,1000,5000,10000]\n",
    "# number of features at every split\n",
    "learning_rate = [1,0.1,0.01,0.001]\n",
    "# create grid\n",
    "params = {\n",
    " 'n_estimators': n_estimators,\n",
    " 'learning_rate': learning_rate,\n",
    " }\n",
    "# Random search of parameters\n",
    "boost_grid = GridSearchCV(estimator = model, param_grid = params, \n",
    "                                cv = 5, verbose=2, scoring='neg_mean_squared_error',n_jobs = -1)\n",
    "# Fit the model\n",
    "boost_grid.fit(X_train, y_train)\n",
    "# print results\n",
    "print(boost_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=GradientBoostingRegressor(n_estimators=1000,max_depth=1,learning_rate=0.01)\n",
    "model.fit(X_train,y_train)\n",
    "y_pred=model.predict(X_test)\n",
    "print('Mean Squared Error:', mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaboost Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "model = AdaBoostRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = [100,1000,5000,10000]\n",
    "# number of features at every split\n",
    "learning_rate = [1,0.1,0.01,0.001]\n",
    "# create grid\n",
    "params = {\n",
    " 'n_estimators': n_estimators,\n",
    " 'learning_rate': learning_rate,\n",
    " }\n",
    "# Random search of parameters\n",
    "boost_grid = GridSearchCV(estimator = model, param_grid = params, \n",
    "                                cv = 5, verbose=2, scoring='neg_mean_squared_error',n_jobs = -1)\n",
    "# Fit the model\n",
    "boost_grid.fit(X_train, y_train)\n",
    "# print results\n",
    "print(boost_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=AdaBoostRegressor(n_estimators=5000,learning_rate=0.01)\n",
    "model.fit(X_train,y_train)\n",
    "y_pred=model.predict(X_test)\n",
    "print('Mean Squared Error:', mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoostRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "model=XGBRegressor()\n",
    "n_estimators = [100,1000,5000,10000]\n",
    "# number of features at every split\n",
    "learning_rate = [1,0.1,0.01,0.001]\n",
    "# create grid\n",
    "params = {\n",
    " 'n_estimators': n_estimators,\n",
    " 'learning_rate': learning_rate,\n",
    " }\n",
    "# Random search of parameters\n",
    "boost_grid = GridSearchCV(estimator = model, param_grid = params, \n",
    "                                cv = 5, verbose=2, scoring='neg_mean_squared_error',n_jobs = -1)\n",
    "# Fit the model\n",
    "boost_grid.fit(X_train, y_train)\n",
    "# print results\n",
    "print(boost_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=XGBRegressor(n_estimators=5000,learning_rate=0.1)\n",
    "model.fit(X_train,y_train)\n",
    "y_pred=model.predict(X_test)\n",
    "print('Mean Squared Error:', mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
